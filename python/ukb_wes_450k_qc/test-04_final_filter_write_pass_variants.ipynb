{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.6.5", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "chrom = None", "metadata": {"tags": ["parameters"]}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "import pyspark\nimport dxpy\nimport hail as hl\nimport pandas as pd\nfrom math import ceil\n\nWD='/opt/notebooks'", "metadata": {"trusted": true}, "execution_count": 1, "outputs": []}, {"cell_type": "code", "source": "my_database = dxpy.find_one_data_object(\n    name=\"my_database\", \n    project=dxpy.find_one_project()[\"id\"]\n)[\"id\"]\ndatabase_dir = f'dnax://{my_database}'\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)\nhl.init(sc=sc, tmp_dir=f'{database_dir}/tmp/')", "metadata": {"trusted": true}, "execution_count": 2, "outputs": [{"name": "stderr", "text": "pip-installed Hail requires additional configuration options in Spark referring\n  to the path to the Hail Python module directory HAIL_DIR,\n  e.g. /path/to/python/site-packages/hail:\n    spark.jars=HAIL_DIR/hail-all-spark.jar\n    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\nSparkUI available at http://ip-10-60-90-238.eu-west-2.compute.internal:8081\nWelcome to\n     __  __     <>__\n    / /_/ /__  __/ /\n   / __  / _ `/ / /\n  /_/ /_/\\_,_/_/_/   version 0.2.78-b17627756568\nLOGGING: writing to /opt/notebooks/hail-20220928-0846-0.2.78-b17627756568.log\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## S0. Define functions, load data", "metadata": {}}, {"cell_type": "code", "source": "def get_gnomad_vcf_path(chrom, blocks):\n    vcf_dir = 'file:///mnt/project/Bulk/Exome sequences_Alternative exome processing/Exome variant call files (gnomAD) (VCFs)'\n    if blocks != '*':\n        blocks = '{'+','.join(map(str, blocks))+'}'\n        \n    return f'{vcf_dir}/ukb24068_c{chrom}_b{blocks}_v1.vcf.gz'\n\n\ndef get_partitioned_chrom(chrom_w_suffix):\n    \"\"\"\n    chrom_w_suffix should be of the form \"{chr}-?of?\", e.g. \"8-1of4\" for partition 1 of 4 in chromosome 8\n    \"\"\"\n    chrom, suffix = chrom_w_suffix.split('-')\n    assert chrom in list(map(str, range(1,23)))+['X','Y'], \"chrom must be in  {1-22, X, Y}\"\n    part_idx, total_parts = map(int, suffix.split('of'))\n    assert (part_idx>=1) & (part_idx<=total_parts)\n    \n    total_vcfs = len(hl.hadoop_ls(get_gnomad_vcf_path(chrom=chrom, blocks=\"*\")))\n    \n    part_size = ceil(total_vcfs/total_parts)\n    \n    start_idx = (part_idx-1)*part_size\n    stop_idx = min((part_idx)*part_size-1, total_vcfs-1)\n    \n    return get_gnomad_vcf_path(chrom, blocks=range(start_idx, stop_idx+1))\n    \n    \n\ndef import_single_chrom_vcf(chrom, blocks = '*'):\n    if 'of' in str(chrom):\n        # Get chunk of chromosome\n        vcf_path = get_partitioned_chrom(chrom_w_suffix=chrom)\n    else:\n        vcf_path = get_gnomad_vcf_path(chrom=chrom, blocks=blocks)\n    \n    return hl.import_vcf(\n        vcf_path, \n        force_bgz=True,\n        reference_genome='GRCh38'\n    )\n\n\ndef get_mad_threshold_tsv_fname(n_mads, classification):\n    return f'ukb_wes_450k.mad_threshold.nmad_{n_mads}.popclass_{classification}.tsv.gz'\n\n\ndef get_pass_mad_threshold_expr(mt, n_mads='4', classification='strict'):\n    mad_fname = get_mad_threshold_tsv_fname(n_mads=n_mads, classification=classification)\n    mad_path = f'file:///mnt/project/data/03_mad_threshold/{mad_fname}'\n#     mad_path = f'file:///opt/notebooks/{mad_fname}'\n    print(mad_path)\n    mad_ht = hl.import_table(\n        mad_path, \n        types={\n            's': hl.tstr, \n            'pass': hl.tbool\n        },\n        key='s',\n        force=True\n    )\n    \n    return mad_ht[mt.s]['pass']\n\ndef get_fail_interval_qc_expr(mt):\n    return mt.info.fail_interval_qc\n\ndef get_lcr_expr(mt):\n    return mt.info.lcr\n\ndef get_segdup_expr(mt):\n    return mt.info.segdup\n\ndef get_filter_contains_rf_expr(mt):\n    return mt.filters.contains('RF')\n\ndef get_inbreeding_coeff(mt):\n    return mt.info.InbreedingCoeff[0]\n\ndef site_filter(mt):\n    # Set genotype to missing if:\n    # - DP < 10\n    # - GQ < 20\n    # - If heterozygous: Alt allele balance <= 0.2\n    \n    SITE_DP_MIN = 10\n    SITE_GQ_MIN = 20\n\n    pass_dp = mt.DP>=SITE_DP_MIN\n    pass_gq = mt.GQ>=SITE_GQ_MIN\n\n    pass_ab_het = mt.GT.is_het() & (mt.AD[1]/mt.DP>0.2)\n    pass_ab = ~mt.GT.is_het() | pass_ab_het\n    mt = mt.filter_entries(pass_dp & pass_gq & pass_ab)\n\n    return mt\n\n\ndef final_variant_filter(mt):\n    # Remove if:\n    # - FILTER row field contains \"RF\" (random forest true positive probability < {threshold})\n    # - Excess heterozygotes (inbreeding coefficient < -0.3)\n    # - Fails gnomAD interval QC\n    # - In low-complexity region\n    # - segdup is true (segment duplication region?)\n    # - No sample has a high quality genotype\n    \n    MIN_INBREEDING_COEFF = -0.3\n    fails_inbreeding_coeff = get_inbreeding_coeff(mt) < MIN_INBREEDING_COEFF\n    \n    # Fail if all genotypes are missing\n    fails_any_hq_genotypes = hl.agg.all(hl.is_missing(mt.GT))\n    \n    return mt.filter_rows(\n        get_filter_contains_rf_expr(mt)\n        | fails_inbreeding_coeff\n        | get_fail_interval_qc_expr(mt)\n        | get_lcr_expr(mt)\n        | get_segdup_expr(mt)\n        | fails_any_hq_genotypes,\n        keep=False\n    )\n\ndef final_variant_filter_skip_hq_gt(mt):\n    # Remove if:\n    # - FILTER row field contains \"RF\" (random forest true positive probability < {threshold})\n    # - Excess heterozygotes (inbreeding coefficient < -0.3)\n    # - Fails gnomAD interval QC\n    # - In low-complexity region\n    # - segdup is true (segment duplication region?)\n    # - No sample has a high quality genotype\n    \n    MIN_INBREEDING_COEFF = -0.3\n    fails_inbreeding_coeff = get_inbreeding_coeff(mt) < MIN_INBREEDING_COEFF\n    \n    # Fail if all genotypes are missing\n    fails_any_hq_genotypes = hl.agg.all(hl.is_missing(mt.GT))\n    \n    return mt.filter_rows(\n        get_filter_contains_rf_expr(mt)\n        | fails_inbreeding_coeff\n        | get_fail_interval_qc_expr(mt)\n        | get_lcr_expr(mt)\n        | get_segdup_expr(mt),\n        keep=False\n    )\n\ndef export_table(ht, fname, out_folder):\n    ht.naive_coalesce(1).export(f'file:///opt/notebooks/{fname}')\n\n    dxpy.upload_local_file(\n        filename=f'/opt/notebooks/{fname}',\n        name=fname,\n        folder=out_folder,\n        parents=True\n    )\n    \ndef export_file(path, out_folder):\n    dxpy.upload_local_file(\n        filename=path,\n        name=path.split('/')[-1],\n        folder=out_folder,\n        parents=True\n    )\n\ndef final_filter(mt):\n    pass_mad_threshold_expr = get_pass_mad_threshold_expr(mt, n_mads='4', classification='strict')\n    mt = mt.filter_cols(pass_mad_threshold_expr)\n\n    mt = site_filter(mt)\n\n    # NOTE: Final variant filter MUST come after site filter in order to remove variants where no individuals have high quality genotypes\n    mt = final_variant_filter(mt)\n    \n    return mt\n\ndef get_final_filter_mt_path(chrom):\n    return f'{database_dir}/04_final_filter_write_to_mt/ukb_wes_450k.qced.chr{chrom}.mt'\n\ndef get_final_filter_count_tsv_fname(chrom):\n    return f'variant_sample_count.final_filter.c{chrom}.tsv'\n\n\ndef export_count_as_tsv(mt, chrom, fname, out_folder):\n    row_ct, col_ct = mt.count()\n    \n    df = pd.DataFrame(data={\n        'row_count': [row_ct], \n        'col_count': [col_ct]\n    })\n    ht = hl.Table.from_pandas(df)\n\n    export_table(\n        ht=ht, \n        fname=fname, \n        out_folder=out_folder\n    )", "metadata": {"trusted": true}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": "chrom=21", "metadata": {"trusted": true}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": "%%time\n\nchrom = chrom\n\nraw = import_single_chrom_vcf(chrom)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## S1. Write list of variants to keep", "metadata": {}}, {"cell_type": "code", "source": "mt = raw\n\n# Write file of variants to keep\nmt = final_variant_filter_skip_hq_gt(mt)\nht = mt.rows().key_by()\nht = ht.select(\n    chrom = ht.locus.contig,\n    position = ht.locus.position,\n    a1 = ht.alleles[0],\n    a2 = ht.alleles[1]\n)\n\nfname=f'ukb_wes_450k.pass_variants.chr{chrom}.tsv.gz'\n\nexport_table(\n    ht=ht,\n    fname=fname,\n    out_folder='/data/04_final_filter'\n)", "metadata": {"trusted": true}, "execution_count": null, "outputs": [{"name": "stderr", "text": "2022-09-27 14:22:45 Hail: INFO: Coerced sorted dataset\n2022-09-27 14:22:45 Hail: INFO: Coerced dataset with out-of-order partitions.\n", "output_type": "stream"}]}]}