{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.6.5", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "chrom = None", "metadata": {"tags": ["parameters"]}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "import pyspark\nimport dxpy\nimport hail as hl\nimport pandas as pd\nfrom math import ceil\n\nWD='/opt/notebooks'\nPLINK_EXPORT_DIR = '/data/07_export_to_plink'", "metadata": {"trusted": true}, "execution_count": 1, "outputs": []}, {"cell_type": "code", "source": "my_database = dxpy.find_one_data_object(\n    name=\"my_database\", \n    project=dxpy.find_one_project()[\"id\"]\n)[\"id\"]\ndatabase_dir = f'dnax://{my_database}'\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)\nhl.init(sc=sc)#, tmp_dir=f'{database_dir}/tmp/')", "metadata": {"trusted": true}, "execution_count": 2, "outputs": [{"name": "stderr", "text": "pip-installed Hail requires additional configuration options in Spark referring\n  to the path to the Hail Python module directory HAIL_DIR,\n  e.g. /path/to/python/site-packages/hail:\n    spark.jars=HAIL_DIR/hail-all-spark.jar\n    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\nSparkUI available at http://ip-10-60-142-10.eu-west-2.compute.internal:8081\nWelcome to\n     __  __     <>__\n    / /_/ /__  __/ /\n   / __  / _ `/ / /\n  /_/ /_/\\_,_/_/_/   version 0.2.78-b17627756568\nLOGGING: writing to /opt/notebooks/hail-20220929-1005-0.2.78-b17627756568.log\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "# Alternative way to initialize Hail\n# From https://github.com/dnanexus/OpenBio/blob/master/hail_tutorial/export_bgen.ipynb\n\n# from pyspark.sql import SparkSession\n\n# builder = (\n#     SparkSession\n#     .builder\n#     .enableHiveSupport()\n# )\n# spark = builder.getOrCreate()\n# hl.init(sc=spark.sparkContext)", "metadata": {"trusted": true}, "execution_count": 2, "outputs": [{"name": "stderr", "text": "pip-installed Hail requires additional configuration options in Spark referring\n  to the path to the Hail Python module directory HAIL_DIR,\n  e.g. /path/to/python/site-packages/hail:\n    spark.jars=HAIL_DIR/hail-all-spark.jar\n    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\nSparkUI available at http://ip-10-60-118-82.eu-west-2.compute.internal:8081\nWelcome to\n     __  __     <>__\n    / /_/ /__  __/ /\n   / __  / _ `/ / /\n  /_/ /_/\\_,_/_/_/   version 0.2.78-b17627756568\nLOGGING: writing to /opt/notebooks/hail-20220928-1343-0.2.78-b17627756568.log\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "## S0. Define functions, load data", "metadata": {}}, {"cell_type": "code", "source": "def get_final_filter_mt_path(chrom):\n    return f'{database_dir}/04_final_filter_write_to_mt/ukb_wes_450k.qced.chr{chrom}.mt'\n\ndef get_final_filter_local_path_prefix(chrom):\n    return f'file:///opt/notebooks/ukb_wes_450k.qced.chr{chrom}'\n\n# def get_final_filter_hadoop_path_prefix(chrom):\n#     return f'{database_dir}/07_export_to_plink/ukb_wes_450k.qced.chr{chrom}'\n\ndef export_local_files(paths, out_folder):\n    '''Export files\n    \n    :param paths: List of strings of path names (do not include 'file://' in front of path)\n    :param out_folder: DNAnexus folder to export to\n    '''\n    if type(paths)!=list:\n        paths = [paths]\n        \n    for path in paths:\n        path = path.replace('file://','')\n        dxpy.upload_local_file(\n            filename=path,\n            name=path.split('/')[-1],\n            folder=out_folder,\n            parents=True\n        )\n        ", "metadata": {"trusted": true}, "execution_count": 3, "outputs": []}, {"cell_type": "code", "source": "mt = hl.read_matrix_table(get_final_filter_mt_path(chrom=chrom))\nmt = mt.rename({'gnomad_info':'info'})", "metadata": {"trusted": true}, "execution_count": 6, "outputs": []}, {"cell_type": "markdown", "source": "## S1. Export to VCF", "metadata": {}}, {"cell_type": "code", "source": "%%time\n\nvcf_path = get_final_filter_local_path_prefix(chrom=chrom)+'.vcf.gz'\n\nhl.export_vcf(\n    dataset = mt,\n    output = vcf_path,\n)", "metadata": {"trusted": true}, "execution_count": 7, "outputs": [{"name": "stderr", "text": "2022-09-29 10:09:07 Hail: INFO: merging 2858 files totalling 74.8G...\n", "output_type": "stream"}, {"name": "stdout", "text": "CPU times: user 87.9 ms, sys: 34.3 ms, total: 122 ms\nWall time: 6min 28s\n", "output_type": "stream"}, {"name": "stderr", "text": "2022-09-29 10:12:32 Hail: INFO: while writing:\n    file:///opt/notebooks/ukb_wes_450k.qced.chr21.vcf.gz\n  merge time: 3m25.4s\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "%%time\n\nexport_local_files(\n    paths=[vcf_path], \n    out_folder='/data/05_export_to_vcf'\n)", "metadata": {"trusted": true}, "execution_count": 8, "outputs": [{"name": "stdout", "text": "CPU times: user 2min 24s, sys: 1min 40s, total: 4min 4s\nWall time: 3min 31s\n", "output_type": "stream"}]}]}